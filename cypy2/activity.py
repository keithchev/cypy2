import os
import re
import sys
import git
import glob
import time
import json
import shutil
import pickle
import datetime
import psycopg2
import subprocess
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib as mpl

from psycopg2 import sql
from scipy import interpolate
from matplotlib import pyplot as plt

from cypy2 import (utils, constants, file_utils, file_settings, constants, dbutils)


class Activity(object):
    '''
    Processing and database-related methods for a single activity
    
    Parameters
    ----------
    source : 'local' or 'db'
    metadata : activity metadata as a pd.Series 


    Public API
    ----------
    from_db(conn, activity_id, kind='metadata'|'raw'|'processed'|'all')
    to_db(conn, kind='raw'|'processed')
    load(conn, kind='raw'|'processed')
    
    # generate (or regenerate) processed records and summary stats
    process()

    # metadata as a pd.Series; generated by LocalActivity._generate_metadata
    metadata

    # summary statistics from the FIT file's 'session' message or generated by self.summarize
    summary(kind='raw|processed')
    
    # events (e.g. starts/stops) from the FIT file's event messages or inferred from the records
    events(kind='raw|processed')

    # time-series data from FIT file record messages
    records(kind='raw|processed')


    TODO
    ----
     - validate metadata in __init__    

    '''


    def __init__(self, metadata, source='db'):

        self.source = source
        self.metadata = metadata

        self._raw_data = None
        self._processed_data = None

        # whether the processed data was loaded from a database
        # or generated by the user calling self.process 
        self._processed_by_user = False

        if self.source not in ['local', 'db']:
            raise ValueError('source must be either \'local\' or \'db\'')


    def process(self):
        '''
        Generate the processed data (records and summary statistics) from the raw data

        TODO: generate summary statistics
        '''
        self._processed_by_user = True
        self._processed_data = {'summary': None, 'records': None}
        self._processed_data['records'] = self.process_records()


    @classmethod
    def from_db(cls, conn, activity_id, kind=None, process_flag=False):
        '''
        Initialize an activity from a cypy2 database
        
        Parameters
        ----------
        conn : psycopg2 connection
        activity_id : the id of the activity to load
        kind : the kind of data to load
            one of None, 'raw', 'processed', or 'all'
            if None, only the activity metadata is loaded

        '''

        # load the metadata from the database
        selector = {'activity_id': activity_id}
        metadata = dbutils.get_rows(conn, 'metadata', selector).iloc[0]

        # instantiate the activity
        activity = cls(metadata, source='db')
        if kind is not None:
            activity.load(conn, kind)

        if kind=='raw' and process_flag:
            activity.process()
        elif kind is not None and process_flag:
            print('Warning: process_flag is true but processed data was loaded from the database')

        return activity


    def load(self, conn, kind='raw'):
        '''
        Load the activity's data from a cypy2 database
        
        Parameters
        ----------
        conn : psycopg2 connection
        kind : the kind(s) of data to load: 'raw', 'processed', or 'all'

        '''
        if kind is None:
            return

        if kind not in ['raw', 'processed', 'all']:
            raise ValueError('%s is not a valid kind of data' % kind)

        if kind in ['raw', 'all']:
            self._raw_data = self._raw_data_from_db(conn)

        if kind in ['processed', 'all']:
            self._processed_data = self._processed_data_from_db(conn)


    def _raw_data_from_db(self, conn):
        '''
        Load the activity's raw data from a cypy2 database

        '''

        selector = {'activity_id': self.metadata.activity_id}

        events = dbutils.get_rows(conn, 'raw_events', selector)
        records = dbutils.get_rows(conn, 'raw_records', selector)

        # records from a one-row dataframe of lists to a dataframe of timepoints
        records = pd.DataFrame(records.to_dict(orient='records').pop())

        # drop activity_id columns
        events.drop('activity_id', axis=1, inplace=True)
        records.drop('activity_id', axis=1, inplace=True)

        # drop record fields with no data
        records.dropna(axis=1, how='all', inplace=True)

        # for now, skip loading the raw summary
        summary = None

        raw_data = {'events': events, 'records': records, 'summary': summary}
        return raw_data


    def _processed_data_from_db(self, conn):
        '''
        Load the activity's most recent processed data from a database

        '''
        
        query = sql.SQL(
            'select * from proc_records where activity_id = {} order by date_created desc limit 1')
        query = query.format(sql.Literal(self.metadata.activity_id)).as_string(conn)

        records = pd.DataFrame(pd.read_sql(query, conn).to_dict(orient='records').pop())
        records.drop(
            ['activity_id', 'date_created', 'date_modified', 'commit_hash', 'geom'], 
            axis=1, 
            inplace=True)

        records.dropna(axis=1, how='all', inplace=True)

        processed_data = {'events': None, 'summary': None, 'records': records}
        return processed_data


    def to_db(self, conn, kind=None, verbose=True):
        '''
        Insert an activity's raw or processed data into a cypy2 database instance

        Parameters
        ----------
        conn : a psycopg2 connection to the database
        kind : the kind of data to insert; either 'raw' or 'processed'

        '''
        assert(kind in ['raw', 'processed'])

        sys.stdout.write('\rInserting %s data for activity %s' % \
            (kind, self.metadata.activity_id))

        if kind=='raw':
            if self.source=='local':
                self._raw_data_to_db(conn, verbose)
            else:
                raise ValueError(
                    'Cannot insert raw data unless the activity was loaded from a local file')

        if kind=='processed':
            self._processed_data_to_db(conn, verbose)


    def _raw_data_to_db(self):
        '''
        Insert/update an activity's raw data in a cypy2 database
        Currently only defined in the LocalActivity subclass, since the raw data should be static
        (that is, an activity loaded from a database should not be able to update its own raw data)

        '''
        raise NotImplementedError('_raw_data_to_db must be defined in subclasses')


    def _processed_data_to_db(self, conn, verbose):
        '''
        Insert an activity's *processed* (that is, derived) data

        Currently, this method always creates a new row in proc_records
        with the latest processed records data, even if the data is unchanged
        (since the table is keyed by (activity_id, date_created)). 

        conn : psycopg2 connection to the database

        '''

        # check whether the processed data exists and was freshly processed
        if self._processed_data is None:
            raise ValueError('No processed data to insert into the database')
        elif not self._processed_by_user and verbose:
            print('Warning: existing processed data was loaded from the database; no need to update')

        # get the current commit in the cypy2 repo
        repo = git.Repo('../')
        current_commit = repo.commit().hexsha

        # warn if there are uncommitted changes in activity.py
        if verbose and 'cypy2/activity.py' in [d.a_path for d in repo.index.diff(None)]:
            print('Warning in Activity.to_db: uncommitted local changes in cypy2/activity.py')

        # ----------------------------------------------------------------------------------------
        #
        # Create a new row in the proc_records table and get its primary key
        #
        # ----------------------------------------------------------------------------------------
        table = 'proc_records'
        activity_id = self.metadata.activity_id
        values = {'activity_id': activity_id, 'commit_hash': current_commit}

        try:
            dbutils.insert_row(conn, table, values)
        except psycopg2.Error as error:
            print('Error inserting data for activity %s:\n%s' % (activity_id, error))
            raise

        # get the new row (assumes we're the only user currently inserting rows)
        row = pd.read_sql(
            'select activity_id, date_created from proc_records order by date_created desc limit 1', conn)
        row = row.iloc[0]

        # sanity check
        assert(row.activity_id==activity_id)

        # the primary key of the new row (which we need in order to populate its remaining columns)
        selector = {'activity_id': activity_id, 'date_created': row.date_created}

        # ----------------------------------------------------------------------------------------
        #
        # Populate the data columns of the new row
        # (the value in each of these columns is an array
        # that corresponds to one column of the records dataframe)
        #
        # ----------------------------------------------------------------------------------------
        records = self.records(kind='processed')
        columns = dbutils.get_column_names(conn, table)
        for column in columns:
            if column in records.columns:
                dbutils.update_value(
                    conn,
                    table=table, 
                    column=column,
                    value=records[column], 
                    selector=selector)
        conn.commit()

        # ----------------------------------------------------------------------------------------
        #
        # Populate the geometry columns of the new row
        # (These columns store the GPS lat/lon coordinates as LineStrings)
        #
        # ----------------------------------------------------------------------------------------
        # there are no lat/lon for indoor rides
        if 'lat' not in records.columns or 'lon' not in records.columns:
            print('Warning: no lat/lon coords for activity %s' % activity_id)
            return

        # note that this column order matters (it corresponds to the order required by LineStringZM)
        coordinates = records[['lon', 'lat', 'altitude', 'elapsed_time']].copy()

        # postGIS converts nulls to zeros, so we have to drop all rows with any nans        
        coordinates.dropna(how='any', axis=0, inplace=True)
        if not coordinates.shape[0]:
            print('Warning: all lat/lon coords for activity %s are missing' % activity_id)
            return

        # GeoJSON of the four-dimensional geometry with elevation and timestamp
        geom = {
            'type': 'LineString',
            'coordinates': coordinates.values.tolist()
        }
        value = json.dumps(geom)

        # note that the hard-coded projection here (4326) must match that specified in the schema
        query = sql.SQL(
            'update proc_records set {column} = {st_force}(ST_SetSRID(ST_GeomFromGeoJSON(%s), 4326))')

        # note that ST_Force2D here implies the 'geom' column must be a LineString
        query_2d = query.format(
            column=sql.Identifier('geom'),
            st_force=sql.SQL('ST_Force2D'))

        # and ST_Force4D implies the 'geom4d' column must be a LineStringZM
        query_4d = query.format(
            column=sql.Identifier('geom4d'),
            st_force=sql.SQL('ST_Force4D'))

        for query in [query_2d, query_4d]:
            full_query = sql.SQL(' ').join([query, dbutils.where_clause(selector)])
            dbutils.execute_query(conn, full_query, (value,), commit=True)


    def process_records(self):
        '''
        Generate processed records from raw records
        
        Does the following:
         - renames/drops some columns
         - calculates elapsed time in seconds
         - interpolates the raw records to a constant sampling rate
         - generates a pause mask from the raw events
         - calculates VAM from altitude
         - various unit conversions

        TODO: calculate additional columns (grade, vert, kJ)

        '''

        records = self.records('raw').reset_index()

        # ----------------------------------------------------------------------------------------
        #
        # temporary hack to correct fitparse bug
        #
        if 'speed' in records.columns:
            speed = records.speed.values
            if max(speed) > 30:
                speed = speed/1000
                records['speed'] = speed

        if 'altitude' in records.columns:
            alt = records.altitude.values
            records['altitude'] = (alt - 2500)/5.
        #
        # ---------------------------------------------------------------------------------------- 

        # column renaming for convenience
        records.rename(columns={'position_lat': 'lat', 'position_long': 'lon'}, inplace=True)

        # drop the enhanced speed and altitude columns, 
        # since they seem to be identical to the 'normal' speed and altitude columns
        # TODO: determine whether this is always true
        dropped_columns = ['enhanced_speed', 'enhanced_altitude']
        for column in dropped_columns:
            if column in records.columns:
                records.drop([column], axis=1, inplace=True)


        # ----------------------------------------------------------------------------------------
        #
        # calculate elapsed time in seconds and drop the timepoint column
        #
        # ----------------------------------------------------------------------------------------
        timestamps = records.timepoint.apply(pd.to_datetime)
        records['elapsed_time'] = [dt.seconds for dt in (timestamps - timestamps[0])]
        records.drop(['timepoint'], axis=1, inplace=True)


        # ----------------------------------------------------------------------------------------
        #
        # interpolate the raw records and calculate the pause mask and VAM
        #
        # ----------------------------------------------------------------------------------------
        records = self._interpolate_records(records, constants.interpolation_timestep)
        records['pause_mask'] = self._calculate_pause_mask(records)

        if 'altitude' in records.columns:
            records['vam'] = self._calculate_vam(records.altitude)


        # ----------------------------------------------------------------------------------------
        #
        # unit conversions
        #
        # ----------------------------------------------------------------------------------------
        # lat/lon coordinates from semicircles to degrees 
        # (note that indoor rides don't have these columns)
        if 'lat' in records.columns:
            records['lat'] *= constants.semicircles_to_degrees
            records['lon'] *= constants.semicircles_to_degrees

        # m/s to mph
        if 'speed' in records.columns:
            records['speed'] *= (constants.miles_per_meter * constants.seconds_per_hour)

        # meters to feet
        if 'altitude' in records.columns:
            records['altitude'] *= constants.feet_per_meter

        # meters to miles
        if 'distance' in records.columns:
            records['distance'] *= constants.miles_per_meter

        return records


    @staticmethod
    def _interpolate_records(records, timestep):
        '''
        Interpolate records to constant one-second sampling

        Note that this is not always necessary, since for many activities,
        the sampling rate is already constant (i.e., not variable/dynamic) 

        TODO: drop *internal* NAs prior to interpolating
              (but retain initial or trailing NAs)

        '''

        timepoints = records.elapsed_time.values
        new_timepoints = np.arange(0, timepoints[-1], timestep)

        new_records = pd.DataFrame()
        new_records['elapsed_time'] = new_timepoints

        # interpolate each column with the new timepoints
        for column in set(records.columns).difference(['elapsed_time']):
            values, mask = records[column].values, records[column].isna().values
            interpolator = interpolate.interp1d(timepoints, values, kind='linear')
            new_records[column] = interpolator(new_timepoints)

        return new_records


    def _calculate_pause_mask(self, records):
        '''
        Generate a pause mask from the pauses implied by in-activity start and stop events 

        Parameters
        ----------
        records : interpolated records

        '''

        # timestamp of the first record 
        # (i.e., the timestamp corresponding to records.elapsed_time==0)
        t0 = pd.to_datetime(self.metadata.records_timestamp)

        # remove initial start time and final stop time
        events = self.events('raw').iloc[1:-1]

        starts = events.loc[events.event_type=='start'].event_time
        stops = events.loc[events.event_type=='stop'].event_time

        elapsed_time = records.elapsed_time.values
        mask =  np.zeros(*elapsed_time.shape)

        for start, stop in zip(starts, stops):
            pause_start, pause_stop = (stop - t0).seconds, (start - t0).seconds
            mask += (elapsed_time > pause_start) & (elapsed_time < pause_stop)

        mask = mask.astype(bool)
        return mask


    def _infer_pauses(self):
        '''
        Infer pauses from the raw records data

        ** currently unused **
        '''

        d = self.records('raw')
        timestamps = d.timepoint.apply(pd.to_datetime)
        elapsed_time = [t.seconds for t in (timestamps - timestamps[0])]

        counts = np.bincount(np.diff(elapsed_time))
        x = np.argwhere(counts).flatten()
        counts = counts[counts > 0]

        pauses = pd.DataFrame(data=list(zip(x, counts)), columns=['timestep', 'frequency'])
        return pauses


    @staticmethod
    def _calculate_vam(altitude):
        '''
        Calculate VAM (vertical ascent velocity in meters per hour)
        using an exponentially weighted linear regression in a sliding window
        
        Empirically, this yields a less noisy estimate of the slope of the altitude
        than the other obvious approach, which is to take a moving average of the derivative

        It also has the advantage that the residuals can be interpreted as a measure of local linearity,
        which may be useful because local *non*linearities usually correspond
        to starts/stops or uphill-downhill transitions

        Parameters
        ----------
        altitude : pd.Series of raw altitude values in meters, indexed by elapsed time
                   **must be interpolated to constant one-second timestep**

        '''

        # check for constant timestep
        assert set(np.diff(altitude.index.values))==set([1])

        # half life of the exponential weights in seconds
        # (a half life of 7sec corresponds to a decay rate of ~10sec)
        halflife = 7
        window_sz = 3*halflife
        alpha = (1 - np.exp(np.log(.5)/halflife))

        # exponentially decaying weights
        weights = (1 - alpha)**(np.arange(0, window_sz, 1))
        weights /= weights.sum()
        weights = weights[::-1]

        y = altitude.values
        x_window = np.arange(window_sz)
        y_windows = utils.sliding_window(y, window_sz, 1)

        slopes, resids = [], []
        for y_window in y_windows:
            slope, offset, res = utils.weighted_linregress(x_window, y_window, weights)
            slopes.append(slope)
            resids.append(res)

        vam, resids = np.array(slopes), np.array(resids)

        # add back the missing initial values
        vam = np.concatenate(([np.nan] * (window_sz - 1), vam))

        # meters per sec to meters per hour
        vam *= constants.seconds_per_hour

        return vam


    def summary(self, kind='raw'):
        '''
        Summary statistics
        '''
        return None


    def events(self, kind='raw'):
        '''
        '''

        if kind=='raw':
            return self._raw_data['events'].copy()
        else:
            raise ValueError('events data is only raw')


    def records(self, kind='raw'):

        if kind=='raw':
            records = self._raw_data['records'].copy()
        elif kind.startswith('proc'):
            records = self._processed_data['records'].copy()
        else:
            raise ValueError('kind must be \'raw\' or \'proc\'')

        return records


    def plot(self, columns=None, overlay=False, xmode='hours', xrange=None, halflife=None):

        colors = sns.color_palette()

        xlabels = {
            'seconds': 'Elapsed time (seconds)', 
            'minutes': 'Elapsed time (minutes)',
            'hours': 'Elapsed time (hours)',
            'miles': 'Distance (miles)',
        }

        # scales when x-axis is time
        xscales = {
            'hours': 3600,
            'minutes': 60,
            'seconds': 1,
        }

        def _style_axis(ax):
            ax.yaxis.grid(b=False)
            ax.xaxis.grid(linestyle='dotted', color=tuple(np.ones(3)*.7))
    
        def _make_pause_rects(starts, stops, y_min, y_max):
            rects = []
            for start, stop in zip(starts, stops):
                rect = mpl.patches.Rectangle((start, y_min), stop - start, y_max - y_min)
                rects.append(rect)
            return rects


        draw_pauses = True
        records = self.records('processed')

        if xmode in ['seconds', 'minutes', 'hours']:
            x = records.elapsed_time.values
            x = x / xscales[xmode]

        elif xmode=='miles':
            x = records.distance.values
            draw_pauses = False

        if not xrange:
            xrange = [0, max(x)]

        mask = (x > min(xrange)) & (x < max(xrange))

        x = x[mask]
        pause_mask = records.pause_mask.values[mask]
        offset = min(xrange)

        if draw_pauses:    
            starts, stops = [
                np.argwhere(np.diff(pause_mask.astype(int))==dp).flatten() for dp in [1, -1]]

            starts, stops = (starts + 1)/xscales[xmode] + offset, (stops + 1)/xscales[xmode] + offset

            if not starts.size and not stops.size:
                draw_pauses = False

            elif (stops.size and not starts.size) or (starts[0] > stops[0]):
                starts = [min(xrange)] + list(starts)

            elif (starts.size and not stops.size) or (stops[-1] < starts[-1]):
                stops = list(stops) + [max(xrange)]


        if isinstance(columns, str):
            columns = [columns]
        
        if overlay:
            fig, left_ax = plt.subplots(1, 1, figsize=(12, 2))
            right_ax = left_ax.twinx()
            axs = [left_ax, right_ax]
        else:
            fig, axs = plt.subplots(len(columns), 1, figsize=(12, 2*len(columns)))
            if not isinstance(axs, np.ndarray):
                axs = [axs]
        
        for column, ax, color in zip(columns, axs, colors[:len(axs)]):
            y = records[column]
            if halflife:
                y = y.ewm(halflife=halflife).mean()
            if column=='vam':
                y[y < 0] = 0
            y = y[mask]

            if draw_pauses:
                rects = _make_pause_rects(starts, stops, y.min(), y.max())
                collection = mpl.collections.PatchCollection(
                    rects, facecolor=np.ones(3)*.9, alpha=.5, edgecolor=None)
                ax.add_collection(collection)

            ax.plot(x, y, color=color, label=column)
            ax.set_ylabel(column)

        for ind, ax in enumerate(axs):
            _style_axis(ax)
            if ind < len(axs) - 1 and not overlay:
                ax.get_xaxis().set_ticklabels([])
        axs[-1].set_xlabel(xlabels[xmode])



class LocalActivity(Activity):
    '''
    A LocalActivity is an activity whose raw data is derived from a local FIT file
    This is a subclass of the Activity base class containing FIT-file-specific parsing methods

    Public methods
    --------------
    from_fit_file
    from_strava_export

    Examples
    --------
    # from a local FIT file
    activity = LocalActivity.from_fit_file('/path/to/fit/file')

    # from a Strava export manager
    manager = StravaExportManager('/path/to/export/', from_cache=True)
    activity = LocalActivity.from_strava_export(data=manager.parsed_data[ind])


    Parameters
    ----------
    fit_data : parsed data from a single FIT file as a dict of dataframes keyed by message name
               must have: 'file_id', 'device_info', 'event', 'session', and 'record'

    strava_metadata : optional one-row dataframe of metadata from a Strava export's activity.csv

    '''

    def __init__(self, fit_data, strava_metadata=None):

        self._fit_data = fit_data
        self._strava_metadata = strava_metadata

        # message types required in raw FIT data
        required_message_names = ['file_id', 'device_info', 'session', 'event', 'record']
        if set(required_message_names).difference(fit_data.keys()):
            raise ValueError('Some message types are missing in fit_data')

        # generate/infer/organize metadata
        metadata = self._generate_metadata(fit_data, strava_metadata)

        # parse/organize the raw data 
        events, summary, records = self._parse_fit_data(fit_data, metadata.activity_id)

        # various consistency checks
        self._validate_fit_data(metadata, records, events)

        # instantiate the activity from the metadata
        super().__init__(metadata, source='local')

        # manually construct the activity's _raw_data
        raw_data = {
            'events': events,
            'summary': summary,
            'records': records,
        }
        self._raw_data = raw_data


    @classmethod
    def from_fit_file(cls, filepath):
        '''
        Initialize an activity directly from a FIT file
    
        **intended for testing only**
        (note that there is no strava_metadata)
        
        Parameters
        filepath : path to a local FIT file

        '''
        
        data = file_utils.parse_fit(filepath)
        activity = cls(data)
        return activity


    @classmethod
    def from_strava_export(cls, data):
        '''
        Initialize an activity from the parsed data cached by managers.StravaExportManager
        
        Parameters
        ----------
        data : dict of message dataframes; usually, this corresponds to one element 
        of the list returned by StravaExportManager.parse_all
        '''
        activity = cls(data, strava_metadata=data['strava_metadata'])
        return activity



    def _raw_data_to_db(self, conn, verbose):
        '''
        Insert or update an activity's *raw* data in a cypy2 database

        Generally, this method should only be called when populating a new database,
        since the raw data should never need to be updated. 
        
        conn : psycopg2 connection to the database

        '''

        activity_id = self.metadata.activity_id

        # ------------------------------------------------------------------------------------
        #
        #  metadata
        #
        # ------------------------------------------------------------------------------------

        # metadata as a (one-row) DataFrame so we can use dbutils.dataframe_to_table
        # TODO: decide how to handle database errors
        metadata = pd.DataFrame(data=[self.metadata])
        try:
            dbutils.dataframe_to_table(conn, 'metadata', metadata, raise_errors=True)
            conn.commit()
        except psycopg2.Error as error:
            print('Error inserting metadata: %s' % error)
            return


        # ------------------------------------------------------------------------------------
        #
        #  events
        #
        # ------------------------------------------------------------------------------------
        events = self.events(kind='raw')
        dbutils.dataframe_to_table(conn, 'raw_events', events, raise_errors=False)
        conn.commit()


        # ------------------------------------------------------------------------------------
        #
        #  summary
        #
        # ------------------------------------------------------------------------------------
        # device summary (i.e., the 'session' message)
        # summary = self.summary(kind='raw')


        # ------------------------------------------------------------------------------------
        #
        #  records
        #
        # ------------------------------------------------------------------------------------
        # create a new row in raw_records for this activity
        dbutils.insert_row(conn, 'raw_records', {'activity_id': activity_id})
        conn.commit()

        records = self.records(kind='raw')

        # note that update_value will overwrite any existing values
        columns = dbutils.get_column_names(conn, 'raw_records')
        for column in columns:
            if column in records.columns:
                dbutils.update_value(
                    conn,
                    table='raw_records', 
                    column=column,
                    value=records[column], 
                    selector={'activity_id': activity_id})

        conn.commit()


    @staticmethod
    def id_from_fit(filepath=None, file_id=None):
        '''
        Generate a unique activity_id from a FIT file,
        using the timestamp in the FIT file's file_id message

        Parameters
        ----------
        filepath : path to a FIT file
        file_id : file_id message as a pandas DataFrame (with a single row) or Series
        
        '''
        if filepath is not None:
            fitfile = file_utils.open_fit(filepath)
            fields = {f.name: f.value for f in next(fitfile.get_messages('file_id')).fields}
            file_id = pd.DataFrame([fields])

        if isinstance(file_id, pd.DataFrame):
            assert(file_id.shape[0]==1)
            file_id = file_id.iloc[0]
        elif isinstance(file_id, pd.Series):
            pass
        else:
            raise ValueError('file_id must be a series or a dataframe')

        timestamp = file_id.time_created
        activity_id = timestamp.strftime('%Y%m%d%H%M%S')
        return activity_id


    @classmethod
    def _generate_metadata(cls, fit_data, strava_metadata=None):
        '''
        Generate metadata from the raw (FIT-file) data

        Metadata columns:        
            - activity id
            - activity type
            - cycling type (indoor/road)
            - FIT file timestamp
            - strava timestamp, title, gear
            - GPS device make and model
            - HRM, power meter, and speed sensor flags

        '''

        # map from FIT-like activity types to strava-like activity types
        activity_type_map = {
            'hiking': 'hike',
            'running': 'run',
            'cycling': 'ride',
            'indoor_cycling': 'indoor',
        }

        def _df_to_series(df):
            if df is None: return None
            assert(df.shape[0]==1)
            return df.iloc[0]

        def _clean_string(val):
            if pd.isna(val): return None
            return str(val).lower().replace(' ', '-')


        # message types we need (we can assume all exist, except for 'sport')
        file_id, device_info, session, sport = \
            fit_data.get('file_id'), fit_data.get('device_info'), fit_data.get('session'), fit_data.get('sport')

        # column renaming in device_info
        device_info.rename(columns={'garmin_product': 'product_name'}, inplace=True)

        # column renaming in file_id
        # nb this is a bit dangerous, because we are assuming that the 'product' column
        # only appears in Wahoo (e.g., non-Garmin) file_id messages;
        file_id.rename(columns={'garmin_product': 'product_name', 'product': 'product_name'}, inplace=True)

        # there should always be only one message (row) for these message types
        file_id, session = _df_to_series(file_id), _df_to_series(session)

        # strava metadata as a series for convenience (it always has one row by definition)
        strava_metadata = _df_to_series(strava_metadata)

        # ------------------------------------------------------------------------------------
        #
        # initialize metadata with activity_id and timestamps 
        # from both the file_id and the first record, which may not be the same
        # (and the timestamp of the first record will be important in, e.g., self._calculate_pause_mask)
        #
        # ------------------------------------------------------------------------------------
        metadata = {
            'activity_id': cls.id_from_fit(file_id=file_id),
            'file_timestamp': str(file_id.time_created),
            'records_timestamp': str(fit_data['record'].iloc[0].timestamp),
        }

        # copy some fields directly from strava metadata
        if strava_metadata is not None:
            metadata.update({
                'filename': strava_metadata['filename'],
                'strava_title': strava_metadata['name'],
                'strava_timestamp': strava_metadata['date'],
            })

        # ------------------------------------------------------------------------------------
        #
        # activity type from either strava metadata or 'sport' field of 'session' message
        # (session.sport is always 'cycling', 'running', or 'hiking')
        #
        # ------------------------------------------------------------------------------------
        if strava_metadata is not None:
            activity_type = strava_metadata.type.lower()
        else:
            activity_type = activity_type_map[session.sport.lower()]
        metadata['activity_type'] = activity_type


        # ------------------------------------------------------------------------------------
        #
        # cycling type from 'sport' message and bike name from strava metadata
        #
        # Note that 'sport' messages *can* have more than one row, ergo the defensive .iloc[0]
        # (so far, this is the case only for 'hiking' activities)
        #
        # ------------------------------------------------------------------------------------ 
        cycling_type, bike_name = None, None
        if activity_type=='ride':
            cycling_type = 'road'
            if sport is not None and sport.iloc[0].sub_sport=='indoor_cycling':
                cycling_type = 'indoor'

            if strava_metadata is not None:
                bike_name = _clean_string(strava_metadata['gear'])

        metadata['cycling_type'] = cycling_type
        metadata['bike_name'] = bike_name


        # ------------------------------------------------------------------------------------
        #
        # device manufacturer and model from file_id
        #
        # ------------------------------------------------------------------------------------
        device_manufacturer = file_id.manufacturer
        if device_manufacturer not in ['garmin', 'wahoo_fitness']:
            print('Warning: unexpected device manufacturer %s' % device_manufacturer)

        if device_manufacturer=='garmin':
            device_model = file_id.product_name

        # hard-coded device model for Wahoo
        if device_manufacturer=='wahoo_fitness':
            device_model = 'elemnt'

        # simplify names
        device_model = 'fenix3' if device_model=='fenix3_hr' else device_model
        device_manufacturer = 'wahoo' if device_manufacturer=='wahoo_fitness' else device_manufacturer

        # check device model
        if device_model not in ['elemnt', 'fr220', 'edge520', 'fenix3']:
            print('Warning: unexpected device model %s' % device_model)

        metadata['device_model'] = device_model
        metadata['device_manufacturer'] = device_manufacturer


        # ------------------------------------------------------------------------------------
        #
        # sensor flags (true when the sensor is present) from device_info
        #
        # Note that using the 'antplus_device_type' column is likely the most robust way
        # of determining which sensors where present, since its values appear to be
        # device-independent. 
        #
        # Notably, the names appearing in the 'product_name' column are not reliable;
        # for example, between 2016-04-23 and 2016-06-17, files from the Edge 520 
        # have no HRM-like value in that column, despite having a row with 'heart_rate'
        # in 'antplus_device_type' as well as a 'heart_rate' column in the record messages.
        #
        # ------------------------------------------------------------------------------------
        hrm_flag, power_flag, speed_flag = False, False, False

        if 'antplus_device_type' in device_info.columns:
            device_types = [s.lower() for s in device_info.antplus_device_type.apply(str).unique()]
            hrm_flag = 'heart_rate' in device_types
            power_flag = 'bike_power' in device_types
            speed_flag = 'bike_speed' in device_types

        metadata.update({
            'heart_rate_flag': hrm_flag, 
            'power_flag': power_flag, 
            'speed_flag': speed_flag
        })
        
        metadata = pd.Series(data=metadata)
        return metadata


    @staticmethod
    def _validate_fit_data(metadata, records, events):
        '''
        Check the integrity of raw (FIT-file) data
        
        TODO: check that required message types exist
        - 
        '''

        activity_id = metadata.activity_id

        # check that we have records for the expected sensors
        # (nb there is always speed data, with or without a sensor)
        for sensor in ['heart_rate', 'power']:

            # skip fenix3 because it has a built-in HRM
            if sensor=='heart_rate' and metadata['device_model']=='fenix3':
                continue

            flag = sensor in records.columns
            if metadata['%s_flag' % sensor] and not flag:
                print('Warning: activity %s has %s sensor but no records' % (activity_id, sensor))
            if not metadata['%s_flag' % sensor] and flag:
                print('Warning: activity %s has %s records but no sensor' % (activity_id, sensor))
        

    @staticmethod
    def _parse_fit_data(fit_data, activity_id):
        '''
        Parse the raw FIT-file data to generate database-ready
        'events', 'summary', and 'record' dataframes

        TODO: parse/cleanup summary and records

        '''
        events = fit_data['event'].copy()
        summary = fit_data['session'].copy()
        records = fit_data['record'].copy()

        # column renaming
        records.rename(columns={'timestamp': 'timepoint'}, inplace=True)

        # for now, keep only the event_type and time columns for 'timer' events (starts and stops)
        events = events.loc[events.event=='timer'][['event_type', 'timestamp']]

        # column names for database
        events = events.rename(columns={'timestamp': 'event_time'})

        events['activity_id'] = activity_id

        # 'stop_all' to 'stop', etc
        events.replace(to_replace=re.compile('stop(.*)$'), value='stop', inplace=True)
        events.replace(to_replace=re.compile('start(.*)$'), value='start', inplace=True)

        # check for pairs of start/stop events that have the same timestamp
        # (this is rare, but real)
        if events.shape[0]!=len(events.event_time.unique()):
            print('Warning: at least two events have the same timestamp')

        # remove any such pairs by finding the index of the first event of each pair
        events = events.reset_index(drop=True)
        inds = np.argwhere(np.array([dt.astype(int) for dt in np.diff(events.event_time)])==0).flatten()
        for ind in inds:
            if set(events.iloc[[ind, ind + 1]].event_type)==set(['start', 'stop']):
                events = events.drop([ind, ind + 1], axis=0)
            else:
                # we should never get here
                print('Warning: two events have the same timestamp *and* type')

        # drop the last event when the last two events are both stops
        # (this usually corresponds to a 'stop' and a 'stop_all')
        events = events.reset_index(drop=True)
        if events.iloc[-1].event_type=='stop' and events.iloc[-2].event_type=='stop':
            events = events.drop(len(events) - 1, axis=0)

        return events, summary, records
